1.
from gensim.downloader import load

print("Loading GloVe (50D)...")
model = load("glove-wiki-gigaword-50")

def word_relationships():
    res1 = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)
    print(f"\nking - man + woman ≈ {res1[0][0]} (similarity: {res1[0][1]:.4f})")

    res2 = model.most_similar(positive=['paris', 'italy'], negative=['france'], topn=1)
    print(f"\nparis - france + italy ≈ {res2[0][0]} (similarity: {res2[0][1]:.4f})")

    print("\nWords similar to 'programming':")
    for word, sim in model.most_similar('programming', topn=5):
        print(f"{word} ({sim:.4f})")

word_relationships()

2.
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from gensim.downloader import load

model = load("glove-wiki-gigaword-50")

words = ['football', 'basketball', 'soccer', 'tennis', 'cricket']
vectors = [model[w] for w in words]
points = PCA(n_components=2).fit_transform(vectors)

for i, word in enumerate(words):
    x, y = points[i]
    plt.scatter(x, y)
    plt.text(x + 0.01, y + 0.01, word)
plt.show()

print("Words similar to 'programming':")
for w, s in model.most_similar('programming', topn=5):
    print(w, s)
3.
from gensim.models import Word2Vec

corpus = [s.split() for s in [
    "The patient was prescribed antibiotics to treat the infection.",
    "The court ruled in favor of the defendant after reviewing the evidence.",
    "Diagnosis of diabetes mellitus requires specific blood tests.",
    "The legal contract must be signed in the presence of a witness.",
    "Symptoms of the disease include fever, cough, and fatigue."
]]

model = Word2Vec(corpus, vector_size=50, window=5, min_count=1, workers=4, epochs=10)

for word in ["patient", "court"]:
    print(f"\nAnalysis for word '{word}'")
    for w, s in model.wv.most_similar(word, topn=5):
        print(w, s)
4.
from gensim.downloader import load
from transformers import pipeline
model, gen = load("glove-wiki-gigaword-50"), pipeline("text-generation", model="gpt2")

enrich = lambda p: ' '.join([w for word in p.split() for w, _ in model.most_similar(word, topn=3)])
op, ep = "lung cancer", enrich("lung cancer")

for p, t in [(op, "Prompt"), (ep, "Enriched Prompt")]:
    print(f"\n{t}:\n{p}")
    print(gen(p, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2)[0]["generated_text"])
5.
from gensim.downloader import load
import random

model = load("glove-wiki-gigaword-50")
iw = "cricket"
words = [w for w, _ in model.most_similar(iw, topn=50)]
random.shuffle(words)
print(f"The topic of {iw} is fascinating, often linked to terms like\n{', '.join(words)}.")
6.
from transformers import pipeline

analyzer = pipeline("sentiment-analysis", model="distilbert/distilbert-base-uncased-finetuned-sst-2-english")
feedbacks = [
    "The product is amazing! I love it!",
    "Terrible service, I am very disappointed.",
    "This is a great experience, I will buy again.",
    "Worst purchase I’ve ever made. Completely dissatisfied.",
    "I'm happy with the quality, but the delivery was delayed."
]

for fb in feedbacks:
    res = analyzer(fb)[0]
    print(f"\nFeedback: {fb}\nSentiment: {res['label']} (Confidence: {res['score']:.2f})")
7.
from transformers import pipeline

summarize = pipeline("summarization", model="facebook/bart-large-cnn")
text = """Natural language processing (NLP) is a field of artificial intelligence 
that focuses on the interaction between computers and humans through natural language. 
The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language 
in a way that is valuable. NLP techniques are used in many applications, such as speech recognition, 
sentiment analysis, machine translation, and chatbot functionality. Machine learning algorithms play a significant role in NLP,
as they help computers to learn from vast amounts of language data and improve their ability to process and generate text.
However, NLP still faces many challenges, such as handling ambiguity, 
understanding context, and processing complex linguistic structures. 
Advances in NLP have been driven by deep learning models, such as transformers, 
which have significantly improved the performance of many NLP tasks."""

print("\nSummarized Text:\n", summarize(text, max_length=150, min_length=50, do_sample=False)[0]['summary_text'])

9.
import wikipediaapi

def fetch(name):
    p = wikipediaapi.Wikipedia('en', headers={"User-Agent": "InfoFetcher"}).page(name)
    if not p.exists(): raise ValueError("Page not found")
    c = p.text
    f = lambda k: next((l.strip() for l in c.split('\n') if k in l.lower()), "N/A")
    return {
        "name": name,
        "founder": f("founder"),
        "founded": f("founded") or f("established"),
        "branches": f("branch"),
        "employees": f("employee"),
        "summary": "\n".join(c.split('\n')[:4])
    }

print(fetch("PESITM"))
10.
import fitz

def extract(f): return "".join(p.get_text() for p in fitz.open(f))
def search(q, t): return [l for l in t.split("\n") if q.lower() in l.lower()] or ["No relevant section found."]

def chatbot():
    ipc = extract("IPC.pdf")
    while True:
        q = input("Ask about IPC (type 'exit' to quit): ")
        if q.lower() == "exit": break
        print("\n".join(search(q, ipc)), "\n" + "-"*50)

chatbot()

